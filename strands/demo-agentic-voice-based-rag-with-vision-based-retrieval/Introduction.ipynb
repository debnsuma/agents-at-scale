{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Integrating Visual Document Intelligence with Voice Response with ColPali, Bedrock and Strands Agents\n",
    "</h1>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2 style=\"text-align: center;\">First Thing First</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"imgs/github.png\" width=\"800\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Challenges with Conventional RAG\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional RAG systems face significant hurdles:\n",
    "\n",
    "1. **Lack of Structural Insight**: These systems often fail to consider the structured presentation of documents, focusing only on text.\n",
    "2. **Fragmented Information Retrieval**: Retrieval processes can be disjointed, missing crucial connections between document parts.\n",
    "3. **Poor Multimodal Integration**: Struggles arise in effectively utilizing diverse data formats within a document.\n",
    "4. **Superficial Retrieval Techniques**: Important details are often overlooked, which are essential for deep understanding.\n",
    "\n",
    "![Challenges with Conventional RAG](imgs/intro/multimodal-rag1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Vision based retrieval\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this tutorial on leveraging vision-driven RAG systems, based  on one of the latest research papers like [ColPali](https://arxiv.org/abs/2407.01449), [ColQwen2](https://huggingface.co/vidore/colqwen2-v0.1), and [Amazon Nova](https://www.aboutamazon.com/news/aws/amazon-nova-artificial-intelligence-bedrock-aws). Before diving into the systems, let's frame our problem statement.\n",
    "\n",
    "Typically, a text-based RAG system works with a corpus of documents to answer queries. \n",
    "\n",
    "But what happens when these documents are not just text but include images, tables, and other formats? In a [previous tutorial](https://github.com/debnsuma/fcc-ai-engineering-aws/blob/main/03-multimodal-rag/01_Multi_modal_RAG_Amazon_Bedrock_Nova.ipynb), we explored how we use external tools, to extract text, images, and tables in the first place before using them as input to the RAG workflow. While effective, this method isn't always optimal.\n",
    "\n",
    "![ColPali System Overview](imgs/intro/img1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is where vision-driven RAG systems come into play. But before we start, let's understand the motivation behind this. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Motivation\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the task of answering questions from a document, similar to a `reading comprehension test`. You're provided with a text and must dig into it to find answers. \n",
    "\n",
    "Like, here given a document(a book) and a question or query (What is universal approximation theorem?), you need to find the answer to the query.\n",
    "\n",
    "![Reading Comprehension](imgs/intro/img2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  How to approach this?\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereâ€™s a step-by-step approach you might take:\n",
    "\n",
    "#### **Step 1: Document Comprehension**\n",
    "- **Grasp the Full Content**: Start by understanding the document's overall structure, including text and visual elements like images, charts and tables, and note the organization of information.\n",
    "\n",
    "![Document Structure](imgs/intro/img3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2: Analyzing the Query**\n",
    "- **Dissect the Query**: Break down the query to determine the exact information needed. For instance, if the query asks about the \"universal approximation theorem,\" identify the key sections related to this term.\n",
    "\n",
    "#### **Step 3: Document Search**\n",
    "- **Targeted Search**: Look for relevant text, diagrams, tables, charts, or summaries that explain complex concepts in both simplified and detailed ways.\n",
    "\n",
    "![Search Process](imgs/intro/img4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4: Integrating Information**\n",
    "- **Link Related Information**: Combine related pieces of information, such as text descriptions with corresponding diagrams, to enhance understanding.\n",
    "\n",
    "![Information Integration](imgs/intro/img5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 5: Crafting a Response**\n",
    "- **Synthesize the Answer**: Compile the information into a comprehensive response, explaining complex concepts clearly and concisely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Challenges with Conventional RAG\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional RAG systems face significant hurdles:\n",
    "\n",
    "1. **Lack of Structural Insight**: These systems often fail to consider the structured presentation of documents, focusing only on text.\n",
    "2. **Fragmented Information Retrieval**: Retrieval processes can be disjointed, missing crucial connections between document parts.\n",
    "3. **Poor Multimodal Integration**: Struggles arise in effectively utilizing diverse data formats within a document.\n",
    "4. **Superficial Retrieval Techniques**: Important details are often overlooked, which are essential for deep understanding.\n",
    "\n",
    "![Challenges with Conventional RAG](imgs/intro/multimodal-rag1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  ColPali: Efficient Document Retrieval with Vision Language Models\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The introduction of [ColPali](https://github.com/illuin-tech/colpali) marks a pivotal advancement in AI, blending vision with language processing to mimic human-like understanding of documents.\n",
    "\n",
    "### **Innovative Features of ColPali**\n",
    "\n",
    "ColPali uses vision-language models (VLMs) to enhance document processing, bypassing traditional text extraction steps and directly analyzing documents as they are.\n",
    "\n",
    "![ColPali Overview](imgs/intro/img6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **How ColPali Enhances Efficiency**\n",
    "\n",
    "1. **Patch Creation**: Documents are divided into manageable image patches, simplifying complex page layouts into smaller, processable units.\n",
    "   \n",
    "   ![Patch Creation](imgs/intro/img7.png)\n",
    "\n",
    "2. **Generating Brain Food**: Each patch is converted into embeddings, rich numerical representations that capture both visual and contextual data.\n",
    "\n",
    "   ![Embedding Process](imgs/intro/img8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Vision Language Model (VLM)\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fully grasp the embedding process in ColPali, it's essential to understand Vision Language Models (VLMs), which excel at integrating visual data with textual annotations. For an in-depth exploration of VLMs, please refer to the [previous tutorial](https://github.com/debnsuma/fcc-ai-engineering-aws/blob/main/02-multimodal-llm/00_Introduction_MultimodalLLM.ipynb).\n",
    "\n",
    "At a high level, VLMs consist of the following key components:\n",
    "\n",
    "- **Image Encoder**: Breaks down images into patches, encoding each into embeddings.\n",
    "- **Text Encoder**: Simultaneously, text data is encoded into its own set of embeddings.\n",
    "\n",
    "![VLM Architecture](imgs/intro/img9.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Image Encoder**\n",
    "The Image Encoder segment of a VLM breaks down images into smaller patches and processes each patch individually to generate embeddings. These embeddings represent the visual data in a format that the model can understand and utilize.\n",
    "\n",
    "- **Patch Processing**: Images are divided into patches, which are then individually fed into the encoder. This modular approach allows the model to focus on detailed aspects of each image segment, facilitating a deeper understanding of the overall visual content.\n",
    "\n",
    "  ![Patch Encoding](imgs/intro/img10.png)\n",
    "\n",
    "- **Adapter Layer Transformation**: After encoding, the output from the image encoder passes through an adapter layer. This layer converts the visual embeddings into a numerical format optimized for further processing within the model.\n",
    "\n",
    "  ![Adapter Layer](imgs/intro/img11.png)\n",
    "\n",
    "### **Text Encoder**\n",
    "Parallel to the image encoding, the Text Encoder processes textual data. It converts text into a set of embeddings that encapsulate the semantic and syntactic nuances of the language.\n",
    "\n",
    "- **Text Processing**: Text is input into the encoder, which then produces embeddings. These embeddings capture the textual context and are crucial for the model to understand and generate language-based responses.\n",
    "\n",
    "  ![Text Encoding](imgs/intro/img12.png)\n",
    "\n",
    "### **Integration and Output Generation**\n",
    "The final stage in the VLM involves integrating the outputs from both the image and text encoders. This integration occurs within a LLM, where both sets of embeddings interact through the Transformer's attention mechanism.\n",
    "\n",
    "- **Contextual Interaction**: The image and text token embeddings are combined and processed through the Transformer model. This interaction allows the model to contextualize the information from both modalities, enhancing its ability to generate accurate and relevant responses based on both text and visual inputs.\n",
    "\n",
    "  ![Final Integration](imgs/intro/img13.png)\n",
    "\n",
    "This comprehensive approach enables VLMs to perform complex tasks that require an understanding of both visual elements and textual information, making them ideal for tasks like multimodal RAG where nuanced document understanding is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  ColPali Embedding Process\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the first step is to divide the document into patches ? So, ColPali, treats each page of the document as an image and creates a set of patches from each page (e.g. 32x32 = 1024 patches per page).\n",
    "\n",
    "![image.png](imgs/intro/img7.png)\n",
    "\n",
    "Starting with image patches, ColPali uniquely encodes each through a vision encoder, then utilizes a Transformer-based LLM to refine these embeddings, bypassing traditional softmax outputs in favor of linear projections. \n",
    "\n",
    "![image.png](imgs/intro/img15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    " Query Time: Bringing It All Together\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At query time, the focus shifts to effectively harnessing precomputed embeddings to find the most relevant document pages quickly and accurately. Here's how the process unfolds:\n",
    "\n",
    "### **Generating and Projecting Tokens**\n",
    "\n",
    "- **Token Generation:** Initially, tokens and their embeddings are generated for the query. This involves transforming the text of the query into a format that the system can process and match against document embeddings.\n",
    "\n",
    "- **Projection:** These tokens are then passed through the same transformer model used during the embedding process. This step involves projecting the tokens into the same embedding space as the document patches, ensuring that the subsequent comparisons are meaningful and accurate.\n",
    "\n",
    "![Query Processing](imgs/intro/img16.png)\n",
    "\n",
    "### **Computing the ColBERT Scoring Matrix**\n",
    "\n",
    "At this point, we have two things:\n",
    "\n",
    "1. Query embeddings\n",
    "2. Embeddings of all pages (at patch level granularity)\n",
    "\n",
    "The next critical step involves computing the ColBERT scoring matrix. Here's how it works:\n",
    "\n",
    "- **Embedding Matchup:** The scoring matrix is essentially a grid where each row corresponds to a query token and each column to a document patch. The entries in the matrix represent the similarity scores, typically calculated as the dot product between the query token embeddings and the document patch embeddings.\n",
    "\n",
    "- **Score Maximization:** For each query token, the system identifies the maximum similarity score across all document patches. This step is crucial because it ensures that the most relevant patches are considered for generating the response.\n",
    "\n",
    "- **Summation for Final Score:** The maximum scores for each query token are then summed up to produce a final score for each document page. This cumulative score represents the overall relevance of the page to the query.\n",
    "\n",
    "![image.png](imgs/intro/img17.png)\n",
    "\n",
    "### **Selecting Top-K Pages**\n",
    "\n",
    "Based on the scores computed:\n",
    "\n",
    "- **Ranking and Retrieval:** The pages are ranked according to their scores, and the top-scoring pages are selected. This selection of top-K pages is crucial as it filters out the pages most likely to contain the information sought by the query.\n",
    "\n",
    "- **Response Generation:** These top pages are then fed, along with the query, into a multimodal language model like Amazon Nova. The model uses both the textual and the visual cues from these pages to generate detailed and contextually accurate responses.\n",
    "\n",
    "![Final Output](imgs/intro/img18.png)\n",
    "\n",
    "If you want to learn more about ColPali, you can refer to the [official documentation](https://github.com/illuin-tech/colpali) and also I would recommend you to read this 9 part blog series on RAG on [DailyDoseofDS](https://www.dailydoseofds.com/) by Avi Chawla and Akshay Pachaar. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ok, enough of theory. Let's see it in action :)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
